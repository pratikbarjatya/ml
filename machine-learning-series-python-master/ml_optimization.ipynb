{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Ensemble Learning\n",
    "\n",
    "Ensemble Learning allows us to combine predictions from different multiple learning algorithms, ultimately making up what we consider to be the *ensemble*. By employing this method, we can have a result with a better predictive performance compared to if we had used a single learner.\n",
    "\n",
    "While this can have positive consequences on performance, it's important to note that one drawback is that there's increased computation time and reduced interpretability. \n",
    "\n",
    "\n",
    "## 3.0 Bagging\n",
    "\n",
    "Bagging is a technique where reuse the same training algorithm several times on different subsets of the training data. \n",
    "\n",
    "### 3.1 Algorithm\n",
    "\n",
    "Given a training dataset D of size N, bagging will generate new training sets D<sub>i</sub> of size M by sampling with replacement from D. Some observations might be repeated in each D<sub>i</sub>. \n",
    "\n",
    "If we set M to N, then on average 63.2% of the original dataset D is represented, the rest will be duplicates.\n",
    "\n",
    "The final step is that we train the classifer C on each C<sub>i</sub> separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Boosting\n",
    "\n",
    "Boosting is an optimization technique that allows us to combine multiple classifiers to improve classification accuracy. In boosting, none of the classifiers just need to be at least slightly better than chance. \n",
    "\n",
    "Boosting involves training classifiers on a subset of the training data that is most informative given the current classifiers. \n",
    "\n",
    "### 4.1 Algorithm\n",
    "\n",
    "The general boosting algorithm first involves fitting a simple model to subsample of the data. Next, we identify misclassified observations (ones that are hard to predict). we focus subsequent learners on these samples to get them right. Lastly, we combine these weak learners to form a more complex but accurate predictor.\n",
    "\n",
    "### 4.2 Boosting in R\n",
    "\n",
    "First, we load the required packages: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 AdaBoosting\n",
    "\n",
    "Now, instead of resampling, we can reweight misclassified training examples:\n",
    "\n",
    "### 5.1 Benefits\n",
    "\n",
    "Aside from its easy implementation, AdaBoosting is great because it's a simple combination of multiple classifiers. These classifiers can also be different. \n",
    "\n",
    "### 5.2 Limits\n",
    "\n",
    "On the other hand, AdaBoost is sensitive to misclassified points in the training data. \n",
    "\n",
    "### 5.3 AdaBoost in R\n",
    "\n",
    "We begin by loading the required package"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
